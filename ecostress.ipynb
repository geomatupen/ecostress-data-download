{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba94b38c-b771-4270-a683-3b11484cf201",
   "metadata": {},
   "source": [
    "## 1. Search for Granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ca47ce2-c2a0-4f31-b5c8-c2025a354315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from urllib.parse import urlparse\n",
    "from dateutil.parser import isoparse\n",
    "import earthaccess as ea\n",
    "\n",
    "def km_to_deg_lat(km: float) -> float:\n",
    "    return km / 110.574\n",
    "\n",
    "def km_to_deg_lon(km: float, lat_deg: float) -> float:\n",
    "    return km / (111.320 * math.cos(math.radians(lat_deg)))\n",
    "\n",
    "def point_radius_to_bbox(lon: float, lat: float, radius_km: float):\n",
    "    dlat = km_to_deg_lat(radius_km)\n",
    "    dlon = km_to_deg_lon(radius_km, lat)\n",
    "    min_lon = max(-180.0, lon - dlon)\n",
    "    max_lon = min( 180.0, lon + dlon)\n",
    "    min_lat = max( -90.0, lat - dlat)\n",
    "    max_lat = min(  90.0, lat + dlat)\n",
    "    return (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "def normalize_temporal(start_date: str, end_date: str):\n",
    "    def to_iso(s, is_start):\n",
    "        if \"T\" in s:\n",
    "            return isoparse(s).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        return f\"{s}T00:00:00Z\" if is_start else f\"{s}T23:59:59Z\"\n",
    "    return (to_iso(start_date, True), to_iso(end_date, False))\n",
    "\n",
    "def first_direct_link(g):\n",
    "    \"\"\"Return the first HTTPS direct data link (not OPeNDAP) if available.\"\"\"\n",
    "    try:\n",
    "        links = g.data_links(access=\"direct\")\n",
    "    except Exception:\n",
    "        links = getattr(g, \"data_links\", []) or []\n",
    "    for href in links:\n",
    "        h = str(href)\n",
    "        if h.startswith(\"https://\") and \"opendap\" not in h.lower():\n",
    "            return h\n",
    "    return None\n",
    "\n",
    "def filename_from_link(href: str) -> str:\n",
    "    if not href:\n",
    "        return \"UNKNOWN_FILENAME\"\n",
    "    return os.path.basename(urlparse(href).path)\n",
    "\n",
    "def safe_umm(g):\n",
    "    if hasattr(g, \"umm\"):\n",
    "        return getattr(g, \"umm\") or {}\n",
    "    try:\n",
    "        return g.get(\"umm\", {})  # type: ignore[attr-defined]\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def safe_temporal(umm: dict):\n",
    "    try:\n",
    "        r = umm.get(\"TemporalExtent\", {}).get(\"RangeDateTime\", {})\n",
    "        return r.get(\"BeginningDateTime\", \"N/A\"), r.get(\"EndingDateTime\", \"N/A\")\n",
    "    except Exception:\n",
    "        return \"N/A\", \"N/A\"\n",
    "\n",
    "def safe_size_mb(umm: dict):\n",
    "    try:\n",
    "        adi = umm.get(\"DataGranule\", {}).get(\"ArchiveAndDistributionInformation\", [])\n",
    "        if adi and \"SizeMBDataGranule\" in adi[0]:\n",
    "            return float(adi[0][\"SizeMBDataGranule\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def print_granule_list(results):\n",
    "    print(\"\\nFound granules:\")\n",
    "    for i, g in enumerate(results, start=1):\n",
    "        href = first_direct_link(g)\n",
    "        fname = filename_from_link(href)\n",
    "        umm = safe_umm(g)\n",
    "        t0, t1 = safe_temporal(umm)\n",
    "        size = safe_size_mb(umm)\n",
    "        size_str = f\"{size:.1f} MB\" if size is not None else \"N/A\"\n",
    "        print(f\"[{i:02d}] {fname} | {t0} → {t1} | Size: {size_str}\")\n",
    "\n",
    "def search_granules(\n",
    "    lat: float,\n",
    "    lon: float,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    search_radius_km: float = 1.0,\n",
    "    product_short_name: str = \"ECO_L2G_LSTE\",\n",
    "    product_version: str = \"002\",\n",
    "    out_dir: str = \"./ecostress_downloads\",\n",
    "):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Logging in to Earthdata (interactive)…\")\n",
    "    ea.login(strategy=\"interactive\")\n",
    "\n",
    "    bbox = point_radius_to_bbox(lon, lat, search_radius_km)\n",
    "    t0, t1 = normalize_temporal(start_date, end_date)\n",
    "\n",
    "    print(f\"\\nSearching {product_short_name} v{product_version}\")\n",
    "    print(f\"Spatial bbox: {bbox}\")\n",
    "    print(f\"Temporal: {t0} to {t1}\")\n",
    "\n",
    "    results = ea.search_data(\n",
    "        short_name=product_short_name,\n",
    "        version=product_version,\n",
    "        temporal=(t0, t1),\n",
    "        bounding_box=bbox,\n",
    "        provider=\"LPCLOUD\",\n",
    "        sort_key=\"-start_date\",\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"No matching granules found.\")\n",
    "        return\n",
    "\n",
    "    print_granule_list(results)\n",
    "\n",
    "    sel = input(\"\\nEnter the index of the granule to download: \").strip()\n",
    "    try:\n",
    "        idx = int(sel) - 1\n",
    "        assert 0 <= idx < len(results)\n",
    "    except Exception:\n",
    "        print(\"Invalid selection.\")\n",
    "        return\n",
    "\n",
    "    chosen = [results[idx]]\n",
    "\n",
    "    print(\"\\nDownloading…\")\n",
    "    paths = ea.download(chosen, out_dir)\n",
    "\n",
    "    h5_path = \"\";\n",
    "    if paths:\n",
    "        print(\"Download complete:\")\n",
    "        for p in paths:\n",
    "            print(\"  -\", p)\n",
    "            h5_path = p; #.split('\\\\')[1];\n",
    "        return h5_path;\n",
    "    else:\n",
    "        print(\"Download returned no files. If you’re on Windows, ensure your browser login succeeded and try again.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2c14d96-623a-4963-a634-9976e69306c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to Earthdata (interactive)…\n",
      "\n",
      "Searching ECO_L2G_LSTE v002\n",
      "Spatial bbox: (-106.96802094336361, 38.91840289267043, -106.94492639663638, 38.936490327329565)\n",
      "Temporal: 2020-08-16T00:00:00Z to 2020-08-18T23:59:59Z\n",
      "\n",
      "Found granules:\n",
      "[01] UNKNOWN_FILENAME | 2020-08-18T20:46:48.667Z → 2020-08-18T20:47:40.636Z | Size: N/A\n",
      "[02] UNKNOWN_FILENAME | 2020-08-18T14:16:22.287Z → 2020-08-18T14:17:14.257Z | Size: N/A\n",
      "[03] UNKNOWN_FILENAME | 2020-08-17T21:34:34.694Z → 2020-08-17T21:35:26.663Z | Size: N/A\n",
      "[04] UNKNOWN_FILENAME | 2020-08-17T21:33:42.724Z → 2020-08-17T21:34:34.693Z | Size: N/A\n",
      "[05] UNKNOWN_FILENAME | 2020-08-17T15:04:27.032Z → 2020-08-17T15:05:19.002Z | Size: N/A\n",
      "[06] UNKNOWN_FILENAME | 2020-08-17T15:03:35.061Z → 2020-08-17T15:04:27.032Z | Size: N/A\n",
      "[07] UNKNOWN_FILENAME | 2020-08-16T22:21:55.918Z → 2020-08-16T22:22:47.888Z | Size: N/A\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the index of the granule to download:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be81fb677a8a4ad184826c793a81b387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded7f8ebe887445fa7a9086aad848ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93699ab9b8e48bbaa4ffde5653e2c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete:\n",
      "  - ecostress_downloads\\ECOv002_L2G_LSTE_11997_006_20200817T150427_0712_01.h5\n"
     ]
    }
   ],
   "source": [
    "LAT = 38.92744661\n",
    "LON = -106.95647367\n",
    "START_DATE = \"2020-08-16\"\n",
    "END_DATE   = \"2020-08-18\"\n",
    "\n",
    "\n",
    "h5_path_txt = search_granules(\n",
    "        LAT,\n",
    "        LON,\n",
    "        START_DATE,\n",
    "        END_DATE,\n",
    "        search_radius_km=1.0,\n",
    "        # product_short_name, product_version, and out_dir use defaults\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630e1ca1-67bc-4982-a237-6a9bdca87812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOv002_L2G_LSTE_11997_006_20200817T150427_0712_01.h5\n"
     ]
    }
   ],
   "source": [
    "print(h5_path_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e969706-8922-43b8-9a4d-e3c89149dda8",
   "metadata": {},
   "source": [
    "## 2. Download Image (celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca215e90-6150-4ace-b424-d8d213141ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "\n",
    "def h5_to_geotiff(\n",
    "    h5_path: str,\n",
    "    out_tif_c: str = None,\n",
    "):\n",
    "    \"\"\"Convert ECOSTRESS LST HDF5 subdataset to GeoTIFF (Celsius).\"\"\"\n",
    "\n",
    "    if not os.path.exists(h5_path):\n",
    "        raise FileNotFoundError(f\"HDF5 file not found: {h5_path}\")\n",
    "\n",
    "    # If output not provided, put next to input\n",
    "    if out_tif_c is None:\n",
    "        base = os.path.splitext(h5_path)[0]\n",
    "        out_tif_c = base + \"_lst_celsius.tif\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(out_tif_c), exist_ok=True)\n",
    "\n",
    "    # 1) Find the LST subdataset\n",
    "    g = gdal.Open(h5_path, gdal.GA_ReadOnly)\n",
    "    if g is None:\n",
    "        raise RuntimeError(\"GDAL could not open the HDF5 file.\")\n",
    "    subs = g.GetSubDatasets()\n",
    "\n",
    "    lst_sds = None\n",
    "    for name, desc in subs:\n",
    "        low = (name + \" \" + desc).lower()\n",
    "        if \"/lst\" in low and \"qc\" not in low:  # pick LST, not QC\n",
    "            lst_sds = name\n",
    "            break\n",
    "    if not lst_sds:\n",
    "        raise RuntimeError(\"LST subdataset not found. Run gdalinfo on the .h5 and share the output.\")\n",
    "\n",
    "    # 2) Open LST subdataset and read data (Kelvin)\n",
    "    src = gdal.Open(lst_sds, gdal.GA_ReadOnly)\n",
    "    arr_k = src.ReadAsArray().astype(\"float32\")\n",
    "\n",
    "    # 3) Mask fill values (common ECOSTRESS fill)\n",
    "    for fv in (-9999.0, -9998.0):\n",
    "        arr_k[arr_k == fv] = np.nan\n",
    "\n",
    "    # 4) Convert to Celsius\n",
    "    arr_c = arr_k - 273.15\n",
    "\n",
    "    # 5) Write GeoTIFF with same georeferencing\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst = driver.Create(\n",
    "        out_tif_c,\n",
    "        src.RasterXSize,\n",
    "        src.RasterYSize,\n",
    "        1,\n",
    "        gdal.GDT_Float32,\n",
    "        options=[\"TILED=YES\", \"COMPRESS=DEFLATE\"]\n",
    "    )\n",
    "    dst.SetGeoTransform(src.GetGeoTransform())\n",
    "    dst.SetProjection(src.GetProjection())\n",
    "    band = dst.GetRasterBand(1)\n",
    "    band.WriteArray(arr_c)\n",
    "    band.SetNoDataValue(np.nan)\n",
    "    band.FlushCache()\n",
    "    dst = None\n",
    "    src = None\n",
    "\n",
    "    print(\"Wrote:\", out_tif_c)\n",
    "    return out_tif_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c3cbf59-a3f5-4014-a7a7-27cd333bc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ecostress_downloads\\ECOv002_L2G_LSTE_11997_006_20200817T150427_0712_01_lst_celsius.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ecostress_downloads\\\\ECOv002_L2G_LSTE_11997_006_20200817T150427_0712_01_lst_celsius.tif'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H5_PATH = r\"ecostress_downloads\\ECOv002_L2G_LSTE_39343_012_20250614T104308_0713_01.h5\"\n",
    "H5_PATH = h5_path_txt\n",
    "\n",
    "h5_to_geotiff(H5_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd04a1-a1f7-4eb0-9b0b-a4c30c12c51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecostress)",
   "language": "python",
   "name": "ecostress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
